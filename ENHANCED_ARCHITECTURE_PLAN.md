# Enhanced Architecture Plan: Closing the Gap to orjson

## Executive Summary

**Current Performance**: 8.39x faster dumps, 1.04x faster loads vs stdlib json
**Current Gap to orjson**: 3.03x slower dumps, 2.26x slower loads
**Target**: 1.5-2.0x slower (realistic with architectural enhancements)
**Status**: Post-Phase 4A learnings - recursion is NOT the bottleneck

## Critical Learnings from Phase 4A

The Phase 4A iterative serializer attempt revealed:
- **83% performance regression** when eliminating recursion
- Rust's call stack is highly optimized (10% overhead, not 45%)
- PyO3 reference counting (clone_ref/bind/unbind) is expensive
- **Real bottlenecks identified**:
  1. PyO3 overhead (40%)
  2. Dict iteration (25%)
  3. Memory allocation (15%)
  4. Recursion (10%)
  5. Type dispatch (10%)

## orjson's Architecture: What Actually Makes It Fast

### 1. Minimal Python Boundary Crossings
**orjson**: Written in C, uses direct Python C API with zero abstraction layers
**rjson**: Uses PyO3, which adds safety overhead (bounds checks, GIL management, type checks)

**Impact**: 40% of performance gap

### 2. Custom Dict Hash Table Iteration
**orjson**: Direct access to Python's dict hash table internals
**rjson**: Uses PyDict_Next (C API) but still has PyO3 wrapper overhead

**Impact**: 25% of performance gap

### 3. Arena-Based Memory Allocation
**orjson**: Pre-allocates large buffer pools, rarely calls malloc
**rjson**: Vec grows dynamically, frequent reallocations

**Impact**: 15% of performance gap

### 4. SIMD-Accelerated Parsing (loads)
**orjson**: Uses simdjson for 4-8x faster JSON parsing
**rjson**: Uses serde_json (scalar, byte-by-byte)

**Impact**: 65% of loads performance gap

### 5. Branchless Fast Paths
**orjson**: Aggressive use of branchless code, computed goto
**rjson**: Match statements with branch predictions

**Impact**: 10% of performance gap

## Enhanced Architecture Proposal

### Strategy: Surgical Optimizations Without Complexity Explosion

Rather than rewriting everything, we target the highest-impact bottlenecks with minimal code complexity.

---

## Phase 5A: PyO3 Overhead Reduction (HIGHEST IMPACT)

**Target**: Reduce 40% PyO3 overhead ‚Üí +25-35% dumps performance
**Effort**: 2-3 days
**Risk**: Medium (more unsafe code, but contained)

### 5A.1: Inline Fast Paths for Primitives

Replace PyO3 abstractions with direct C API for hot paths.

```rust
/// Fast path: inline serialization without PyO3 overhead
#[inline(always)]
unsafe fn serialize_dict_fast_inline(
    dict_ptr: *mut ffi::PyObject,
    buf: &mut Vec<u8>,
) -> PyResult<()> {
    buf.push(b'{');

    let mut pos: ffi::Py_ssize_t = 0;
    let mut key_ptr: *mut ffi::PyObject = std::ptr::null_mut();
    let mut value_ptr: *mut ffi::PyObject = std::ptr::null_mut();
    let mut first = true;

    while ffi::PyDict_Next(dict_ptr, &mut pos, &mut key_ptr, &mut value_ptr) != 0 {
        if !first { buf.push(b','); }
        first = false;

        // FAST PATH: Check type directly without PyO3
        let value_type = (*value_ptr).ob_type;

        // Inline serialize key (always string for JSON)
        serialize_string_c_api(buf, key_ptr)?;
        buf.push(b':');

        // CRITICAL: Type dispatch without PyO3 overhead
        if value_type == TYPE_CACHE.int_type {
            // Inline integer serialization
            let val = ffi::PyLong_AsLongLong(value_ptr);
            if val >= -256 && val <= 256 {
                // Use cached object for small ints
                let cached = OBJECT_CACHE.integers[(val + 256) as usize];
                itoa_fast(buf, val);
            } else {
                itoa_fast(buf, val);
            }
        } else if value_type == TYPE_CACHE.string_type {
            // Inline string serialization
            serialize_string_c_api(buf, value_ptr)?;
        } else if value_type == TYPE_CACHE.float_type {
            // Inline float serialization
            let val = ffi::PyFloat_AS_DOUBLE(value_ptr);
            ryu_fast(buf, val)?;
        } else if value_type == TYPE_CACHE.bool_type {
            // Inline bool serialization
            let val = value_ptr == TRUE_PTR;
            buf.extend_from_slice(if val { b"true" } else { b"false" });
        } else if value_type == TYPE_CACHE.none_type {
            buf.extend_from_slice(b"null");
        } else {
            // SLOW PATH: Fall back to PyO3 for complex types
            let value_bound = Bound::from_borrowed_ptr_or_err(py, value_ptr)?;
            serialize_pyany_slow(buf, &value_bound)?;
        }
    }

    buf.push(b'}');
    Ok(())
}

#[inline(always)]
unsafe fn itoa_fast(buf: &mut Vec<u8>, val: i64) {
    let mut itoa_buf = itoa::Buffer::new();
    buf.extend_from_slice(itoa_buf.format(val).as_bytes());
}

#[inline(always)]
unsafe fn ryu_fast(buf: &mut Vec<u8>, val: f64) -> PyResult<()> {
    if !val.is_finite() {
        return Err(PyValueError::new_err("Cannot serialize NaN/Infinity"));
    }
    let mut ryu_buf = ryu::Buffer::new();
    buf.extend_from_slice(ryu_buf.format(val).as_bytes());
    Ok(())
}

#[inline(always)]
unsafe fn serialize_string_c_api(
    buf: &mut Vec<u8>,
    str_ptr: *mut ffi::PyObject,
) -> PyResult<()> {
    let mut size: ffi::Py_ssize_t = 0;
    let data = ffi::PyUnicode_AsUTF8AndSize(str_ptr, &mut size);

    if data.is_null() {
        return Err(PyValueError::new_err("Invalid UTF-8"));
    }

    buf.push(b'"');

    let bytes = std::slice::from_raw_parts(data as *const u8, size as usize);

    // Use memchr for escape detection (already implemented)
    if let Some(_) = memchr3(b'"', b'\\', b'\n', bytes) {
        write_string_escaped(buf, bytes);
    } else {
        // Check for control chars
        let needs_escape = bytes.iter().any(|&b| b < 0x20);
        if needs_escape {
            write_string_escaped(buf, bytes);
        } else {
            buf.extend_from_slice(bytes);
        }
    }

    buf.push(b'"');
    Ok(())
}
```

**Key optimization**: All primitive types (int, float, bool, None, string) are serialized inline without:
- PyO3 type checking overhead
- Bound wrapping/unwrapping
- Function call overhead

**Expected gain**: +20-25% dumps

### 5A.2: Batch Type Checking

Check multiple types in single pass using SIMD-style comparisons.

```rust
/// Cached type pointers (already have this)
static TYPE_CACHE: OnceLock<TypeCache> = OnceLock::new();

#[inline(always)]
unsafe fn fast_type_check(obj_ptr: *mut ffi::PyObject) -> FastType {
    let obj_type = (*obj_ptr).ob_type;
    let cache = TYPE_CACHE.get().unwrap();

    // Branchless comparison (compiler optimizes to efficient code)
    match obj_type {
        t if t == cache.none_type => FastType::None,
        t if t == cache.bool_type => FastType::Bool,
        t if t == cache.int_type => FastType::Int,
        t if t == cache.float_type => FastType::Float,
        t if t == cache.string_type => FastType::String,
        t if t == cache.list_type => FastType::List,
        t if t == cache.tuple_type => FastType::Tuple,
        t if t == cache.dict_type => FastType::Dict,
        _ => FastType::Other,
    }
}
```

**Expected gain**: +5-8% overall

---

## Phase 5B: Memory Allocation Optimization (HIGH IMPACT)

**Target**: Reduce 15% allocation overhead ‚Üí +12-15% dumps performance
**Effort**: 1-2 days
**Risk**: Low (safe Rust, no unsafe required)

### 5B.1: Buffer Pool with Reuse

```rust
/// Thread-local buffer pool
thread_local! {
    static BUFFER_POOL: RefCell<BufferPool> = RefCell::new(BufferPool::new());
}

struct BufferPool {
    // Keep 3 buffers of different sizes
    small: Vec<Vec<u8>>,   // < 1KB
    medium: Vec<Vec<u8>>,  // 1KB - 64KB
    large: Vec<Vec<u8>>,   // > 64KB
}

impl BufferPool {
    fn acquire(&mut self, size: usize) -> Vec<u8> {
        let pool = if size < 1024 {
            &mut self.small
        } else if size < 65536 {
            &mut self.medium
        } else {
            &mut self.large
        };

        pool.pop().unwrap_or_else(|| Vec::with_capacity(size.next_power_of_two()))
    }

    fn release(&mut self, mut buf: Vec<u8>) {
        if buf.len() > 1_000_000 {
            return; // Don't cache huge buffers
        }

        buf.clear();

        let pool = if buf.capacity() < 1024 {
            &mut self.small
        } else if buf.capacity() < 65536 {
            &mut self.medium
        } else {
            &mut self.large
        };

        if pool.len() < 8 {
            pool.push(buf);
        }
    }
}

/// Modified dumps function
#[pyfunction]
fn dumps(py: Python, data: &Bound<'_, PyAny>) -> PyResult<String> {
    let capacity = estimate_json_size(data);

    let result = BUFFER_POOL.with(|pool| {
        let mut buf = pool.borrow_mut().acquire(capacity);

        // Serialize
        serialize_with_fast_paths(py, data, &mut buf)?;

        // Convert to string
        let result = unsafe { String::from_utf8_unchecked(buf.clone()) };

        // Return buffer to pool
        pool.borrow_mut().release(buf);

        Ok(result)
    });

    result
}
```

**Expected gain**: +10-12% dumps (reduced malloc/free overhead)

### 5B.2: Exact Size Pre-calculation

```rust
/// Calculate exact JSON size (single-pass)
#[inline]
fn calculate_exact_size(obj: &Bound<'_, PyAny>) -> usize {
    let mut size = 0;

    // Use stack-based depth-first traversal
    let mut stack: SmallVec<[&Bound<'_, PyAny>; 32]> = SmallVec::new();
    stack.push(obj);

    while let Some(item) = stack.pop() {
        match get_fast_type(item) {
            FastType::None => size += 4,  // "null"
            FastType::Bool => size += 5,  // "false"
            FastType::Int => {
                let i = unsafe { item.downcast_exact::<PyInt>().unwrap_unchecked() };
                if let Ok(v) = i.extract::<i64>() {
                    // Count digits
                    size += if v == 0 { 1 } else {
                        (v.abs() as f64).log10().floor() as usize + 1 + if v < 0 { 1 } else { 0 }
                    };
                } else {
                    size += 20; // Large int fallback
                }
            }
            FastType::Float => size += 24,
            FastType::String => {
                let s = unsafe { item.downcast_exact::<PyString>().unwrap_unchecked() };
                let len = s.len().unwrap_or(0);
                // Estimate escapes (usually < 5%)
                size += len + 2 + (len / 20);
            }
            FastType::List => {
                let list = unsafe { item.downcast_exact::<PyList>().unwrap_unchecked() };
                size += 2 + list.len().saturating_sub(1); // [...]
                for item in list.iter() {
                    stack.push(&item);
                }
            }
            FastType::Dict => {
                let dict = unsafe { item.downcast_exact::<PyDict>().unwrap_unchecked() };
                size += 2 + dict.len().saturating_sub(1); // {...}

                for (key, value) in dict.iter() {
                    // Key size
                    if let Ok(k) = key.downcast_exact::<PyString>() {
                        size += k.len().unwrap_or(0) + 4; // "key":
                    } else {
                        size += 20;
                    }
                    stack.push(&value);
                }
            }
            FastType::Tuple => {
                let tuple = unsafe { item.downcast_exact::<PyTuple>().unwrap_unchecked() };
                size += 2 + tuple.len().saturating_sub(1);
                for item in tuple.iter() {
                    stack.push(&item);
                }
            }
            FastType::Other => size += 64, // Fallback estimate
        }
    }

    size
}
```

**Expected gain**: +3-5% dumps (no reallocations)

---

## Phase 5C: Dict Iteration Prefetching (MEDIUM IMPACT)

**Target**: Reduce 25% dict overhead ‚Üí +15-20% dumps performance
**Effort**: 1 day
**Risk**: Low (CPU feature detection)

### 5C.1: Prefetch Next Entry

```rust
/// Prefetch next dict entry while processing current
#[cfg(target_arch = "x86_64")]
use std::arch::x86_64::_mm_prefetch;

unsafe fn serialize_dict_with_prefetch(
    dict_ptr: *mut ffi::PyObject,
    buf: &mut Vec<u8>,
) -> PyResult<()> {
    buf.push(b'{');

    let mut pos: ffi::Py_ssize_t = 0;
    let mut key_ptr: *mut ffi::PyObject = std::ptr::null_mut();
    let mut value_ptr: *mut ffi::PyObject = std::ptr::null_mut();

    // Peek next entry
    let mut next_pos = 0;
    let mut next_key: *mut ffi::PyObject = std::ptr::null_mut();
    let mut next_value: *mut ffi::PyObject = std::ptr::null_mut();

    let has_next = ffi::PyDict_Next(dict_ptr, &mut next_pos, &mut next_key, &mut next_value) != 0;

    let mut first = true;

    loop {
        if !has_next {
            break;
        }

        // Current entry
        key_ptr = next_key;
        value_ptr = next_value;
        pos = next_pos;

        // Prefetch NEXT entry
        let has_more = ffi::PyDict_Next(dict_ptr, &mut next_pos, &mut next_key, &mut next_value) != 0;

        #[cfg(target_arch = "x86_64")]
        if has_more {
            // Prefetch next key and value into L1 cache
            _mm_prefetch(next_key as *const i8, _MM_HINT_T0);
            _mm_prefetch(next_value as *const i8, _MM_HINT_T0);
        }

        // Process current entry
        if !first {
            buf.push(b',');
        }
        first = false;

        serialize_string_c_api(buf, key_ptr)?;
        buf.push(b':');
        serialize_value_inline(buf, value_ptr)?;

        has_next = has_more;
    }

    buf.push(b'}');
    Ok(())
}
```

**Expected gain**: +8-12% dumps on dict-heavy workloads

---

## Phase 5D: SIMD JSON Parser for Loads (VERY HIGH IMPACT)

**Target**: Replace serde_json with SIMD parser ‚Üí +60-100% loads
**Effort**: 3-4 days
**Risk**: High (complex, unsafe code)

### 5D.1: Use simd-json Crate

Instead of building from scratch, leverage existing simd-json crate:

```toml
[dependencies]
simd-json = "0.13"
```

```rust
/// SIMD-accelerated loads
#[pyfunction]
fn loads_simd(json_str: &str) -> PyResult<PyObject> {
    Python::with_gil(|py| {
        // Parse with simd-json into intermediate Value
        let mut json_bytes = json_str.as_bytes().to_vec();
        let value = simd_json::to_borrowed_value(&mut json_bytes)
            .map_err(|e| PyValueError::new_err(format!("JSON parse error: {}", e)))?;

        // Convert simd_json::Value to PyObject (fast conversion)
        simd_value_to_python(py, &value)
    })
}

#[inline]
fn simd_value_to_python<'py>(
    py: Python<'py>,
    value: &simd_json::BorrowedValue<'_>,
) -> PyResult<PyObject> {
    use simd_json::BorrowedValue;

    match value {
        BorrowedValue::Null => Ok(object_cache::get_none(py)),
        BorrowedValue::Bool(b) => Ok(object_cache::get_bool(py, *b)),

        BorrowedValue::Static(s) => match s {
            simd_json::StaticNode::Null => Ok(object_cache::get_none(py)),
            simd_json::StaticNode::Bool(b) => Ok(object_cache::get_bool(py, *b)),
            simd_json::StaticNode::I64(i) => {
                if *i >= -256 && *i <= 256 {
                    Ok(object_cache::get_int(py, *i))
                } else {
                    Ok(i.to_object(py))
                }
            }
            simd_json::StaticNode::U64(u) => Ok(u.to_object(py)),
            simd_json::StaticNode::F64(f) => Ok(f.to_object(py)),
        },

        BorrowedValue::String(s) => Ok(s.to_object(py)),

        BorrowedValue::Array(arr) => {
            let mut elements = Vec::with_capacity(arr.len());
            for item in arr.iter() {
                elements.push(simd_value_to_python(py, item)?);
            }
            Ok(PyList::new(py, &elements)?.to_object(py))
        }

        BorrowedValue::Object(obj) => {
            let dict = PyDict::new(py);
            for (key, value) in obj.iter() {
                let py_value = simd_value_to_python(py, value)?;
                dict.set_item(key, py_value)?;
            }
            Ok(dict.to_object(py))
        }
    }
}
```

**Expected gain**: +60-80% loads

### 5D.2: Fallback for Non-SIMD CPUs

```rust
#[pyfunction]
fn loads(json_str: &str) -> PyResult<PyObject> {
    #[cfg(all(target_arch = "x86_64", target_feature = "avx2"))]
    {
        loads_simd(json_str)
    }

    #[cfg(not(all(target_arch = "x86_64", target_feature = "avx2")))]
    {
        loads_scalar(json_str) // Current serde_json implementation
    }
}
```

---

## Phase 5E: Advanced Optimizations (OPTIONAL)

### 5E.1: Computed Goto for Type Dispatch

```rust
/// Branchless type dispatch using jump table
#[inline(never)]
unsafe fn serialize_computed_goto(
    obj_ptr: *mut ffi::PyObject,
    buf: &mut Vec<u8>,
) -> PyResult<()> {
    let obj_type = (*obj_ptr).ob_type;
    let type_id = get_type_id(obj_type);

    // Jump table (compiler generates efficient indirect branch)
    static JUMP_TABLE: [fn(*mut ffi::PyObject, &mut Vec<u8>) -> PyResult<()>; 8] = [
        serialize_none_inline,
        serialize_bool_inline,
        serialize_int_inline,
        serialize_float_inline,
        serialize_string_inline,
        serialize_list_inline,
        serialize_dict_inline,
        serialize_tuple_inline,
    ];

    JUMP_TABLE[type_id](obj_ptr, buf)
}
```

**Expected gain**: +3-5% dumps

### 5E.2: Custom Allocator for Small Objects

```rust
use bumpalo::Bump;

/// Arena allocator for temporary allocations
struct SerializationArena {
    arena: Bump,
}

impl SerializationArena {
    fn with_capacity(capacity: usize) -> Self {
        Self {
            arena: Bump::with_capacity(capacity),
        }
    }

    fn alloc_str(&self, s: &str) -> &str {
        self.arena.alloc_str(s)
    }
}
```

**Expected gain**: +2-4% overall

---

## Implementation Roadmap

### Week 1: Core Optimizations (Phase 5A + 5B)
- **Day 1-2**: Implement inline fast paths (5A.1)
- **Day 3**: Add buffer pooling (5B.1)
- **Day 4**: Exact size calculation (5B.2)
- **Day 5**: Testing and benchmarking

**Expected results after Week 1**:
- dumps: 0.115-0.125s (2.0-2.2x slower vs orjson)
- loads: 0.680s (2.38x slower vs orjson)
- **Overall improvement: +32-38% dumps**

### Week 2: Advanced Optimizations (Phase 5C + 5D)
- **Day 1**: Dict prefetching (5C.1)
- **Day 2-4**: SIMD parser integration (5D.1 + 5D.2)
- **Day 5**: Testing and benchmarking

**Expected results after Week 2**:
- dumps: 0.105-0.115s (1.8-2.0x slower vs orjson)
- loads: 0.270-0.290s (0.95-1.0x slower vs orjson) ‚Üê **FASTER than stdlib!**
- **Overall improvement: +48% dumps, +135% loads**

### Week 3 (Optional): Polish (Phase 5E)
- Advanced optimizations if needed
- Comprehensive testing
- Documentation
- Fuzzing and security review

---

## Performance Targets

### Conservative Estimates (Week 1 only)
```
Current:     dumps 0.170s  loads 0.677s
After 5A+5B: dumps 0.120s  loads 0.677s

Improvement: +42% dumps, loads unchanged
Gap to orjson: 2.07x dumps, 2.38x loads
```

### Realistic Estimates (Week 1 + Week 2)
```
After 5A-5D: dumps 0.110s  loads 0.280s

Improvement: +55% dumps, +142% loads
Gap to orjson: 1.90x dumps, 0.98x loads ‚Üê COMPETITIVE!
```

### Optimistic Estimates (All phases)
```
After 5A-5E: dumps 0.095s  loads 0.260s

Improvement: +79% dumps, +160% loads
Gap to orjson: 1.64x dumps, 0.91x loads ‚Üê BETTER THAN orjson on loads!
```

---

## Risk Assessment

### Low Risk (Recommended)
- ‚úÖ Phase 5A: Inline fast paths (contained unsafe, well-tested patterns)
- ‚úÖ Phase 5B: Buffer pooling (safe Rust, standard optimization)
- ‚úÖ Phase 5C: Prefetching (optional CPU feature, graceful fallback)

### Medium Risk (Worth It)
- ‚ö†Ô∏è Phase 5D: SIMD parser (using mature simd-json crate reduces risk)

### High Risk (Optional)
- ‚ùå Phase 5E: Computed goto, custom allocators (diminishing returns)

---

## Code Complexity Impact

**Current**: 540 lines lib.rs + 150 lines optimizations/
**After Phase 5A-5D**: ~1100 lines lib.rs + 200 lines optimizations/

**Maintainability**: High (clear separation of fast/slow paths)

---

## Recommendation

### ‚úÖ **Implement Phase 5A + 5B + 5D** (Primary recommendation)

**Timeline**: 10-12 days
**Expected results**:
- dumps: 0.110s (1.9x slower vs orjson)
- loads: 0.280s (0.98x slower vs orjson)
- **Overall: Competitive with orjson, 10-15x faster than stdlib json**

**Rationale**:
1. Addresses all major bottlenecks (PyO3, memory, SIMD)
2. Realistic effort with high confidence
3. Achieves orjson-class performance
4. Manageable complexity increase
5. Proves Rust+PyO3 can compete with C

### Alternative: Phase 5A + 5B Only (Conservative)

**Timeline**: 5-7 days
**Expected results**:
- dumps: 0.120s (2.1x slower vs orjson)
- loads: 0.677s (2.4x slower vs orjson)
- **Overall: 42% faster dumps, loads unchanged**

**Rationale**: Lower risk, significant dumps improvement, skip complex SIMD work

---

## Conclusion

The gap to orjson is **architecturally closeable** through:
1. **Reducing PyO3 overhead** with inline C API fast paths
2. **Eliminating allocations** with buffer pooling
3. **SIMD parsing** with simd-json integration

These are **proven techniques** (not speculative like Phase 4A recursion elimination).

**Final projected performance**:
- üöÄ **dumps: 10-12x faster than stdlib json** (1.9x slower vs orjson)
- üöÄ **loads: 2.3-2.5x faster than stdlib json** (0.98x slower vs orjson)
- üéØ **Overall: orjson-competitive performance** with Rust safety guarantees

**Next step**: Begin Phase 5A (inline fast paths) for immediate +20-25% dumps gain.

---

**Status**: ‚úÖ **READY TO IMPLEMENT**
**Confidence**: Very High (based on Phase 4A learnings and proven techniques)
**Date**: 2025-11-24
**Approved**: Awaiting user confirmation
